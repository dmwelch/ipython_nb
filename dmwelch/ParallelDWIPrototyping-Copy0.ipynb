{
 "metadata": {
  "name": "ParallelDWIPrototyping-Copy0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Purpose\n",
      "=======\n",
      "\n",
      "The purpose of this pipeline is to complete all the pre-processing steps needed to turn diffusion-weighted images into FA images that will be used to build a template diffusion tensor atlas for fiber tracking.\n",
      "\n",
      "Inputs\n",
      "======\n",
      "The input to this pipeline is a list of subject IDs that is used to generate lists of the corresponding DWIs processed with automated quality control, T2s, and brain label images that are treated as brain masks.\n",
      "\n",
      "Pipeline Steps for CreateDWIWorkflow\n",
      "====================================\n",
      "1. A rigid transform from the b0 of the DWI to the T2 is first derived with BRAINSFit. This rigid transform is then used to resample the DWI in place into the physical space of the T2 (with gtractResampleDWIInPlace) while preserving the voxel lattice of the DWI.\n",
      "\n",
      "1. The b0 from the DWI resampled in place is extracted with extractNrrdVectorIndex. A BSpline transform from the T2 to the b0 of the DWI resampled in place is then derived with BRAINSFit and used to resample the brain mask into the the space of the DWI resampled in place with BRAINSResample.\n",
      "\n",
      "1. A masked tensor image is estimated with dtiprocess using the DWI resampled in place and resampled brain mask. dtiprocess is used again to compute FA, MD, RD, Frobenius norm, lambda1 (AD), lambda2, and lambda3 images with the masked tensor image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import glob\n",
      "import sys\n",
      "\n",
      "#\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\n",
      "#####################################################################################\n",
      "#     Prepend the shell environment search paths\n",
      "PROGRAM_PATHS = '/raid0/homes/johnsonhj/src/BSA-clang31/bin'\n",
      "PROGRAM_PATHS = PROGRAM_PATHS.split(':')\n",
      "PROGRAM_PATHS.extend(os.environ['PATH'].split(':'))\n",
      "os.environ['PATH'] = ':'.join(PROGRAM_PATHS)\n",
      "\n",
      "CUSTOM_ENVIRONMENT=dict()\n",
      "    \n",
      "CLUSTER_QUEUE_LONG = '-q OSX'\n",
      "CLUSTER_QUEUE= '-q OSX'\n",
      "    \n",
      "\n",
      "    \n",
      "# Platform specific information\n",
      "#     Prepend the python search paths\n",
      "PYTHON_AUX_PATHS = '/raid0/homes/johnsonhj/src/BRAINSStandAlone/AutoWorkup:/raid0/homes/johnsonhj/src/BSA-clang31/SimpleITK-build/XXXWrapping/:/raid0/homes/johnsonhj/src/BSA-clang31/NIPYPE'\n",
      "PYTHON_AUX_PATHS = PYTHON_AUX_PATHS.split(':')\n",
      "PYTHON_AUX_PATHS.extend(sys.path)\n",
      "sys.path = PYTHON_AUX_PATHS\n",
      "\n",
      "import SimpleITK as sitk\n",
      "import nipype\n",
      "from nipype.interfaces.base import CommandLine, CommandLineInputSpec, TraitedSpec, File, Directory\n",
      "from nipype.interfaces.base import traits, isdefined, BaseInterface\n",
      "from nipype.interfaces.utility import Merge, Split, Function, Rename, IdentityInterface\n",
      "import nipype.interfaces.io as nio   # Data i/oS\n",
      "import nipype.pipeline.engine as pe  # pypeline engine\n",
      "from nipype.interfaces.freesurfer import ReconAll\n",
      "from SEMTools import *\n",
      "\n",
      "def get_global_sge_script(pythonPathsList, binPathsList, customEnvironment={}):\n",
      "    \"\"\"This is a wrapper script for running commands on an SGE cluster\n",
      "so that all the python modules and commands are pathed properly\"\"\"\n",
      "\n",
      "    custEnvString = \"\"\n",
      "    for key, value in customEnvironment.items():\n",
      "        custEnvString += \"export \" + key + \"=\" + value + \"\\n\"\n",
      "\n",
      "    PYTHONPATH = \":\".join(pythonPathsList)\n",
      "    BASE_BUILDS = \":\".join(binPathsList)\n",
      "    GLOBAL_SGE_SCRIPT = \"\"\"#!/bin/bash\n",
      "echo \"STARTED at: $(date +'%F-%T')\"\n",
      "echo \"Ran on: $(hostname)\"\n",
      "export PATH={BINPATH}\n",
      "export PYTHONPATH={PYTHONPATH}\n",
      "\n",
      "echo \"========= CUSTOM ENVIORNMENT SETTINGS ==========\"\n",
      "echo \"export PYTHONPATH={PYTHONPATH}\"\n",
      "echo \"export PATH={BINPATH}\"\n",
      "echo \"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\"\n",
      "\n",
      "echo \"With custom environment:\"\n",
      "echo {CUSTENV}\n",
      "{CUSTENV}\n",
      "## NOTE:  nipype inserts the actual commands that need running below this section.\n",
      "\"\"\".format(PYTHONPATH=PYTHONPATH, BINPATH=BASE_BUILDS, CUSTENV=custEnvString)\n",
      "    return GLOBAL_SGE_SCRIPT\n",
      "\n",
      "## Create the shell wrapper script for ensuring that all jobs running on remote hosts from SGE\n",
      "#  have the same environment as the job submission host.\n",
      "JOB_SCRIPT = get_global_sge_script(sys.path, PROGRAM_PATHS, CUSTOM_ENVIRONMENT)\n",
      "SGE_JOB_SCRIPT=JOB_SCRIPT\n",
      "\n",
      "def GetDWIReferenceImagesFromSessionID(SESSION_TUPLE,BASE_STRUCT,BASE_DWI):\n",
      "    \"\"\"A function to extract file names from base parameters\"\"\"\n",
      "    import os\n",
      "    import glob\n",
      "    PROJ_ID=SESSION_TUPLE[0]\n",
      "    SUBJ_ID=SESSION_TUPLE[1]\n",
      "    SESSION_ID=SESSION_TUPLE[2]\n",
      "    FixImageList=glob.glob(\"{BASE_STRUCT}/{PROJ_ID}/{SUBJ_ID}/{SESSION_ID}/TissueClassify/t2_average_BRAINSABC.nii.gz\".format(BASE_STRUCT=BASE_STRUCT,PROJ_ID=PROJ_ID,SUBJ_ID=SUBJ_ID,SESSION_ID=SESSION_ID))\n",
      "    FixMaskImageList=glob.glob(\"{BASE_STRUCT}/{PROJ_ID}/{SUBJ_ID}/{SESSION_ID}/TissueClassify/fixed_brainlabels_seg.nii.gz\".format(BASE_STRUCT=BASE_STRUCT,PROJ_ID=PROJ_ID,SUBJ_ID=SUBJ_ID,SESSION_ID=SESSION_ID))\n",
      "    MovingDWIList=glob.glob(\"{BASE_DWI}/{PROJ_ID}/{SUBJ_ID}/{SESSION_ID}/*_concat_QCed.nrrd\".format(BASE_DWI=BASE_DWI,PROJ_ID=PROJ_ID,SUBJ_ID=SUBJ_ID,SESSION_ID=SESSION_ID))\n",
      "    \n",
      "    ## Should check that each list has 1 element\n",
      "    \n",
      "    print \"^\"*80\n",
      "    print SESSION_TUPLE\n",
      "    print BASE_STRUCT\n",
      "    print BASE_DWI\n",
      "    print \"^\"*80\n",
      "    print FixImageList\n",
      "    print FixMaskImageList\n",
      "    print MovingDWIList\n",
      "    print \"^\"*80\n",
      "    FixImage=FixImageList[0]\n",
      "    FixMaskImage=FixMaskImageList[0]\n",
      "    MovingDWI=MovingDWIList[0]\n",
      "    \n",
      "\n",
      "    print \"=\"*80\n",
      "    print FixImage\n",
      "    print FixMaskImage\n",
      "    print MovingDWI\n",
      "    print \"=\"*80\n",
      "    \n",
      "    return PROJ_ID, SUBJ_ID, SESSION_ID, FixImage, FixMaskImage, MovingDWI\n",
      "\n",
      "\n",
      "def MergeByExtendListElements(FAImageList):\n",
      "    ## Initial list with empty dictionaries\n",
      "    \n",
      "    ListOfImagesDictionaries=list()\n",
      "    for ff in FAImageList:\n",
      "        ListOfImagesDictionaries.append( { 'FA': ff, 'DUMMY': ff } )\n",
      "\n",
      "    ## HACK:  Need to make it so that AVG_AIR.nii.gz is has a background value of 1\n",
      "    registrationImageTypes = ['FA']  # ['T1','T2'] someday.\n",
      "    # DefaultContinuousInterpolationType='LanczosWindowedSinc' ## Could also be Linear for speed.\n",
      "    DefaultContinuousInterpolationType = 'Linear'\n",
      "    DTIinterpolationType = 'ResampleDTIlogEuclidean'\n",
      "    \n",
      "    \"\"\"\n",
      "    ***********\n",
      "    Here's the deal: the interpolationMapping is supposed to define the type of interpolation done in antsRegistration(), \n",
      "    NOT the function to call in the registration node.\n",
      "    ***********\n",
      "    \"\"\"\n",
      "    interpolationMapping = {'T1': DefaultContinuousInterpolationType,\n",
      "                            'T2': DefaultContinuousInterpolationType,\n",
      "                            'PD': DefaultContinuousInterpolationType,\n",
      "                            'FL': DefaultContinuousInterpolationType,\n",
      "                            'FA': DefaultContinuousInterpolationType,\n",
      "                            'DUMMY': DefaultContinuousInterpolationType,\n",
      "                            'BRAINMASK': 'MultiLabel',\n",
      "                            'DTI': DTIinterpolationType}\n",
      "    return ListOfImagesDictionaries,registrationImageTypes,interpolationMapping\n",
      "\n",
      "\n",
      "def CreateDWIWorkFlow(proj_name, subj_name, session_name, sessionCount):  \n",
      "    WFname=\"DWIPrototype_\"+str(proj_name)+str(subj_name)+\"_\"+str(session_name)\n",
      "    DWIWorkflow = pe.Workflow(name=WFname)\n",
      "    #DWIWorkflow.base_dir = os.path.join(CACHE_BASE,session_name)\n",
      "    \n",
      "    inputsSpec = pe.MapNode(interface=IdentityInterface(fields=['proj_id', 'subj_id', 'session_id','T2Volume', 'DWIVolume','BrainMask']),\n",
      "                                                        iterfield=['proj_id', 'subj_id', 'session_id','T2Volume', 'DWIVolume','BrainMask'],\n",
      "                                                        name='inputspec')\n",
      "    \n",
      "    outputsSpec = pe.Node(interface=IdentityInterface(fields=['FAImage','MDImage','RDImage','FrobeniusNormImage','Lambda1Image','Lambda2Image','Lambda3Image']),\n",
      "                                                      name='outputspec')\n",
      "    \n",
      "    BFitB0_T2 = pe.MapNode(interface=BRAINSFit(), iterfield=['fixedVolume','movingVolume'], name=\"B0ToT2_Rigid\")\n",
      "    #BF_cpu_sge_options_dictionary = {'qsub_args': '-S /bin/bash -pe smp1 2-12 -l h_vmem=14G,mem_free=4G -o /dev/null -e /dev/null ' + CLUSTER_QUEUE, 'overwrite': True}\n",
      "    \n",
      "    #BFitB0_T2.plugin_args = BF_cpu_sge_options_dictionary\n",
      "    BFitB0_T2.inputs.costMetric = \"MMI\"\n",
      "    BFitB0_T2.inputs.numberOfSamples = 100000\n",
      "    BFitB0_T2.inputs.numberOfIterations = [1500]\n",
      "    BFitB0_T2.inputs.numberOfHistogramBins = 50\n",
      "    BFitB0_T2.inputs.maximumStepLength = 0.2\n",
      "    BFitB0_T2.inputs.minimumStepLength = [0.00005]\n",
      "    BFitB0_T2.inputs.useRigid = True\n",
      "    #BFitB0_T2.inputs.useAffine = True  # Using initial transform from BRAINSABC\n",
      "    BFitB0_T2.inputs.maskInferiorCutOffFromCenter = 65\n",
      "    BFitB0_T2.inputs.maskProcessingMode = \"ROIAUTO\"\n",
      "    BFitB0_T2.inputs.ROIAutoDilateSize = 13\n",
      "    BFitB0_T2.inputs.backgroundFillValue = 0.0\n",
      "    BFitB0_T2.inputs.initializeTransformMode = 'useCenterOfHeadAlign'\n",
      "    \n",
      "    BFitB0_T2.inputs.outputTransform = \"B0ToT2_RigidTransform.h5\"\n",
      "    BFitB0_T2.inputs.outputVolume = \"B0_in_T2Space_Output.nii.gz\"\n",
      "    \n",
      "    DWIWorkflow.connect(inputsSpec, 'T2Volume', BFitB0_T2, 'fixedVolume')\n",
      "    DWIWorkflow.connect(inputsSpec, 'DWIVolume', BFitB0_T2, 'movingVolume')\n",
      "    \n",
      "    DWIRIP = pe.MapNode(interface=gtractResampleDWIInPlace(),\n",
      "                        iterfield=['inputTransform','inputVolume'],\n",
      "                        name=\"DWIRIP_B0ToT2\")\n",
      "    DWIRIP.inputs.outputVolume = 'DTIPRepOutput_RIP.nrrd'\n",
      "    #DWIRIP.inputs.imageOutputSize = [164,164,100]\n",
      "    \n",
      "    DWIWorkflow.connect(BFitB0_T2,'outputTransform',DWIRIP,'inputTransform')\n",
      "    DWIWorkflow.connect(inputsSpec, 'DWIVolume',DWIRIP,'inputVolume')\n",
      "    \n",
      "    BSPLINE_T2_TO_RIPB0 = pe.MapNode(interface=BRAINSFit(), iterfield=['fixedVolume','movingVolume'],name=\"BSPLINE_T2_TO_RIPB0\")\n",
      "    #BSPLINE_T2_TO_RIPB0.plugin_args = BF_cpu_sge_options_dictionary\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.costMetric = \"MMI\"\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.numberOfSamples = 100000\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.numberOfIterations = [1500]\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.numberOfHistogramBins = 50\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.maximumStepLength = 0.2\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.minimumStepLength = [0.00025,0.00025,0.00025,0.00025,0.00025]\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useRigid = True\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useScaleVersor3D = True\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useScaleSkewVersor3D = True\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useAffine = True  # Using initial transform from BRAINSABC\n",
      "    \n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useBSpline = True\n",
      "    #BSPLINE_T2_TO_RIPB0.inputs.useROIBSpline = True\n",
      "    \n",
      "    ##  This needs to be debugged, it should work. BSPLINE_T2_TO_RIPB0.inputs.useROIBSpline = True\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useExplicitPDFDerivativesMode = \"AUTO\"\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.useCachingOfBSplineWeightsMode = \"ON\"\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.maxBSplineDisplacement = 24\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.splineGridSize = [ 14, 10, 12 ]\n",
      "    \n",
      "    BSPLINE_T2_TO_RIPB0.inputs.maskInferiorCutOffFromCenter = 65\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.maskProcessingMode = \"ROIAUTO\"\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.ROIAutoDilateSize = 13\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.backgroundFillValue = 0.0\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.initializeTransformMode = 'useCenterOfHeadAlign'\n",
      "    \n",
      "    \n",
      "    BSPLINE_T2_TO_RIPB0.inputs.bsplineTransform = \"T2ToRIPB0_BSplineTransform.h5\"\n",
      "    BSPLINE_T2_TO_RIPB0.inputs.outputVolume = \"T2ToRIPB0_Output.nii.gz\"\n",
      "    \n",
      "    DWIWorkflow.connect(DWIRIP, 'outputVolume', BSPLINE_T2_TO_RIPB0, 'fixedVolume')\n",
      "    DWIWorkflow.connect(inputsSpec, 'T2Volume', BSPLINE_T2_TO_RIPB0, 'movingVolume')\n",
      "    \n",
      "    EXTRACT_B0 = pe.MapNode(interface=extractNrrdVectorIndex(),iterfield=['inputVolume'],name=\"EXTRACT_B0\")\n",
      "    EXTRACT_B0.inputs.vectorIndex = 0\n",
      "    EXTRACT_B0.inputs.outputVolume = 'B0_Image.nrrd'\n",
      "    DWIWorkflow.connect(DWIRIP,'outputVolume',EXTRACT_B0,'inputVolume')\n",
      "    \n",
      "    RESAMPLE_BRAINMASK = pe.MapNode(interface=BRAINSResample(),iterfield=['warpTransform','inputVolume','referenceVolume'], name=\"RESAMPLE_BRAINMASK\")\n",
      "    RESAMPLE_BRAINMASK.inputs.interpolationMode = 'NearestNeighbor' # This needs to be debugged'Binary'\n",
      "    RESAMPLE_BRAINMASK.inputs.outputVolume = 'DeformedBrainMaskDWIRIP.nrrd'\n",
      "    RESAMPLE_BRAINMASK.inputs.pixelType = 'uchar'\n",
      "    \n",
      "    DWIWorkflow.connect(BSPLINE_T2_TO_RIPB0,'bsplineTransform',RESAMPLE_BRAINMASK,'warpTransform')\n",
      "    DWIWorkflow.connect(inputsSpec, 'BrainMask',RESAMPLE_BRAINMASK,'inputVolume')\n",
      "    DWIWorkflow.connect(EXTRACT_B0,'outputVolume',RESAMPLE_BRAINMASK,'referenceVolume')\n",
      "    \n",
      "    DTIEstim = pe.MapNode(interface=dtiestim(),iterfield=['dwi_image','brain_mask'], name=\"DTIEstim_Process\")\n",
      "    DTIEstim.inputs.method = \"wls\"\n",
      "    DTIEstim.inputs.tensor_output = 'DTI_Output.nrrd'\n",
      "    DWIWorkflow.connect(DWIRIP, 'outputVolume', DTIEstim, 'dwi_image')\n",
      "    DWIWorkflow.connect(RESAMPLE_BRAINMASK, 'outputVolume', DTIEstim, 'brain_mask')\n",
      "    \n",
      "    DTIProcess = pe.MapNode(interface=dtiprocess(),iterfield=['dti_image'], name=\"DTIProcess\")\n",
      "    DTIProcess.inputs.fa_output = \"FA.nrrd\"\n",
      "    DTIProcess.inputs.md_output = \"MD.nrrd\"\n",
      "    DTIProcess.inputs.RD_output = \"RD.nrrd\"\n",
      "    DTIProcess.inputs.frobenius_norm_output = \"frobenius_norm_output.nrrd\"\n",
      "    DTIProcess.inputs.lambda1_output = \"lambda1_output.nrrd\"\n",
      "    DTIProcess.inputs.lambda2_output = \"lambda2_output.nrrd\"\n",
      "    DTIProcess.inputs.lambda3_output = \"lambda3_output.nrrd\"\n",
      "    DTIProcess.inputs.scalar_float = True\n",
      "    \n",
      "    DWIWorkflow.connect(DTIEstim,'tensor_output',DTIProcess,'dti_image')\n",
      "    DWIWorkflow.connect(DTIProcess,'fa_output',outputsSpec,'FAImage')\n",
      "    DWIWorkflow.connect(DTIProcess,'md_output',outputsSpec,'MDImage')\n",
      "    DWIWorkflow.connect(DTIProcess,'RD_output',outputsSpec,'RDImage')\n",
      "    DWIWorkflow.connect(DTIProcess,'frobenius_norm_output',outputsSpec,'FrobeniusNormImage')\n",
      "    DWIWorkflow.connect(DTIProcess,'lambda1_output',outputsSpec,'Lambda1Image')\n",
      "    DWIWorkflow.connect(DTIProcess,'lambda2_output',outputsSpec,'Lambda2Image')\n",
      "    DWIWorkflow.connect(DTIProcess,'lambda3_output',outputsSpec,'Lambda3Image')\n",
      "    \n",
      "    DWIDataSink = pe.Node(interface=nio.DataSink(), name='DWIDataSink')\n",
      "    DWIDataSink.inputs.base_directory = '/hjohnson/HDNI/20130214_DWIPROCESSING_NIPYPE'\n",
      "    DWIDataSink.inputs.container = \"DWIPrototype_Results\"\n",
      "    # HACK: OutputSpec is returning a LIST for each, so we need to split them to the DataSink\n",
      "    # DWIDataSink.iterables = ('substitutions', list(zip([str(item) for item in range(sessionCount)], [str(item) for item in range(sessionCount)])))\n",
      "    \n",
      "    def sinkRegex(proj_id, subj_id, session_id):\n",
      "        import os.path\n",
      "        return [('/*/*/DTIProcess/*/_DTIProcess[0-9]*', \n",
      "                 os.path.join(proj_id[0], subj_id[0], session_id[0]))]\n",
      "    \n",
      "    DWIWorkflow.connect([(inputsSpec, DWIDataSink, \n",
      "                          [(('proj_id', sinkRegex, 'subj_id', 'session_id'), 'regexp_substitutions')])])\n",
      "    DWIWorkflow.connect(outputsSpec,'FAImage', DWIDataSink, 'Output.@FAImage')\n",
      "    DWIWorkflow.connect(outputsSpec,'MDImage', DWIDataSink, 'Output.@MDImage')\n",
      "    DWIWorkflow.connect(outputsSpec,'RDImage', DWIDataSink, 'Output.@RDImage')\n",
      "    DWIWorkflow.connect(outputsSpec,'FrobeniusNormImage', DWIDataSink, 'Output.@FrobeniusNormImage')\n",
      "    DWIWorkflow.connect(outputsSpec,'Lambda1Image', DWIDataSink, 'Output.@Lambda1Image')\n",
      "    DWIWorkflow.connect(outputsSpec,'Lambda2Image', DWIDataSink, 'Output.@Lambda2Image')\n",
      "    DWIWorkflow.connect(outputsSpec,'Lambda3Image', DWIDataSink, 'Output.@Lambda3Image')\n",
      "    DWIWorkflow.connect(DTIEstim, 'tensor_output', DWIDataSink, 'Output.@tensor_output')\n",
      "    \n",
      "\n",
      "    return DWIWorkflow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## This cell is to  provide information specific to the subjects that need to be processed in our lab.\n",
      "SESSIONS_TO_PROCESS=[('PHD_024','0003','42245'),('PHD_024','0005','33071')]\n",
      "\"\"\"                 ,('PHD_024','0029','84091'),\n",
      "                     ('PHD_024','0091','60387'),('PHD_024','0091','78867'),('PHD_024','0093','50120'),\n",
      "                     ('PHD_024','0093','60307'),('PHD_024','0093','88775'),('PHD_024','0122','42742'),\n",
      "                     ('PHD_024','0122','63892'),('PHD_024','0131','76658'),('PHD_024','0131','90863'),\n",
      "                     ('PHD_024','0132','38235'),('PHD_024','0132','43991'),('PHD_024','0132','74443'),\n",
      "                     ('PHD_024','0133','63793'),('PHD_024','0133','81826'),('PHD_024','0137','11834'),\n",
      "                     ('PHD_024','0138','84460'),('PHD_024','0140','31352')]\"\"\"\n",
      "BASE_STRUCT = '/paulsen/Experiments/20130202_PREDICTHD_Results'\n",
      "BASE_DWI ='/paulsen/Experiments/20130211_DWICONCAT'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The desired behavior would be to use a MapNode here"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MasterWFname=\"ManySubjectDWIPrototype\"\n",
      "MasterDWIWorkflow = pe.Workflow(name=MasterWFname)\n",
      "\"\"\"\n",
      "***********\n",
      "Changed the cache directory\n",
      "***********\n",
      "\"\"\"\n",
      "BASE_DIR = os.path.join('/hjohnson/HDNI/20130214_DWIPROCESSING_NIPYPE/DWI_CACHE/DAVE_test',\"MasterWFname\")\n",
      "MasterDWIWorkflow.base_dir = BASE_DIR\n",
      "\n",
      "MasterDWIWorkflow.config['execution'] = {\n",
      "        'plugin': 'Linear',\n",
      "        #'stop_on_first_crash':'true',\n",
      "        #'stop_on_first_rerun': 'true',\n",
      "        'stop_on_first_crash': 'false',\n",
      "        'stop_on_first_rerun': 'false',  # This stops at first attempt to rerun, before running, and before deleting previous results.\n",
      "        'hash_method': 'timestamp',\n",
      "        'single_thread_matlab': 'true',  # Multi-core 2011a  multi-core for matrix multiplication.\n",
      "        'remove_unnecessary_outputs': 'false',\n",
      "        'use_relative_paths': 'false',  # relative paths should be on, require hash update when changed.\n",
      "        'remove_node_directories': 'false',  # Experimental\n",
      "        'local_hash_check': 'true',\n",
      "        'job_finished_timeout': 45\n",
      "    }\n",
      "MasterDWIWorkflow.config['logging'] = {\n",
      "        'workflow_level': 'DEBUG',\n",
      "        'filemanip_level': 'DEBUG',\n",
      "        'interface_level': 'DEBUG',\n",
      "        'log_directory': BASE_DIR\n",
      "    }\n",
      "\n",
      "\n",
      "if True:\n",
      "    ## I can't figure out how to make this map node work, so resorting to a stupid for loop\n",
      "    GetFileNamesNode = pe.MapNode(interface=Function(function=GetDWIReferenceImagesFromSessionID,\n",
      "                            input_names=['SESSION_TUPLE','BASE_STRUCT','BASE_DWI'],\n",
      "                            output_names=['PROJ_ID', 'SUBJ_ID', 'SESSION_ID','FixImage','FixMaskImage','MovingDWI']),\n",
      "                            iterfield = ['SESSION_TUPLE'],\n",
      "                            run_without_submitting=True, name=\"99_GetDWIReferenceImagesFromSessionID\")\n",
      "\n",
      "    GetFileNamesNode.inputs.SESSION_TUPLE = SESSIONS_TO_PROCESS\n",
      "    GetFileNamesNode.inputs.BASE_STRUCT = BASE_STRUCT\n",
      "    GetFileNamesNode.inputs.BASE_DWI = BASE_DWI\n",
      "\n",
      "    #ID_interface =  pe.Node(interface=IdentityInterface(fields=['T2Volume', 'DWIVolume','BrainMask'],\n",
      "    #                    iterfield=['T2Volume', 'DWIVolume','BrainMask']),\n",
      "    #                    name='ID_interface')\n",
      "    #MasterDWIWorkflow.connect(GetFileNamesNode,'FixImage',ID_interface,'T2Volume')\n",
      "    #MasterDWIWorkflow.connect(GetFileNamesNode,'FixMaskImage',ID_interface,'BrainMask')\n",
      "    #MasterDWIWorkflow.connect(GetFileNamesNode,'MovingDWI',ID_interface,'DWIVolume')\n",
      "\n",
      "    myDWIWorkflow = CreateDWIWorkFlow(\"ALL_PROJ\",\"ALL_SUBJ\",\"ALL_SESSION\", len(SESSIONS_TO_PROCESS))\n",
      "    MasterDWIWorkflow.connect(GetFileNamesNode,'FixImage',myDWIWorkflow,'inputspec.T2Volume')\n",
      "    MasterDWIWorkflow.connect(GetFileNamesNode,'FixMaskImage',myDWIWorkflow,'inputspec.BrainMask')\n",
      "    MasterDWIWorkflow.connect(GetFileNamesNode,'MovingDWI',myDWIWorkflow,'inputspec.DWIVolume')\n",
      "    MasterDWIWorkflow.connect(GetFileNamesNode,'PROJ_ID', myDWIWorkflow,'inputspec.proj_id')\n",
      "    MasterDWIWorkflow.connect(GetFileNamesNode,'SUBJ_ID', myDWIWorkflow,'inputspec.subj_id')\n",
      "    MasterDWIWorkflow.connect(GetFileNamesNode,'SESSION_ID', myDWIWorkflow,'inputspec.session_id')\n",
      "    \"\"\"\n",
      "else:\n",
      "    all_workflows = dict()\n",
      "    GetFileNamesNode =dict()\n",
      "    \n",
      "    length_of_sessions = len(SESSIONS_TO_PROCESS)\n",
      "    MergeFAsNode = pe.Node(interface=Merge( length_of_sessions ),\n",
      "                           run_without_submitting=True,\n",
      "                           name=\"MergeFAsNode\")\n",
      "    index = 1\n",
      "    for singleSession in SESSIONS_TO_PROCESS:\n",
      "        localProjectID=singleSession[0]\n",
      "        localSubjectID=singleSession[1]\n",
      "        localSessionID=singleSession[2]\n",
      "        all_workflows[localSessionID] = CreateDWIWorkFlow(localProjectID,localSubjectID,localSessionID)\n",
      "        GetFileNamesNodeName='99_GetDWIReferenceImagesFromSessionID_'+str(localSessionID)\n",
      "        GetFileNamesNode[localSessionID] = pe.Node(interface=Function(function=GetDWIReferenceImagesFromSessionID,\n",
      "                            input_names=['SESSION_TUPLE','BASE_STRUCT','BASE_DWI'],\n",
      "                            output_names=['FixImage','FixMaskImage','MovingDWI']),\n",
      "                            run_without_submitting=True, name=GetFileNamesNodeName)\n",
      "        GetFileNamesNode[localSessionID].inputs.SESSION_TUPLE = singleSession\n",
      "        GetFileNamesNode[localSessionID].inputs.BASE_STRUCT = BASE_STRUCT\n",
      "        GetFileNamesNode[localSessionID].inputs.BASE_DWI = BASE_DWI\n",
      "        \n",
      "        MasterDWIWorkflow.connect(GetFileNamesNode[localSessionID],'FixImage',all_workflows[localSessionID],'inputspec.T2Volume')\n",
      "        MasterDWIWorkflow.connect(GetFileNamesNode[localSessionID],'FixMaskImage',all_workflows[localSessionID],'inputspec.BrainMask')\n",
      "        MasterDWIWorkflow.connect(GetFileNamesNode[localSessionID],'MovingDWI',all_workflows[localSessionID],'inputspec.DWIVolume')\n",
      "        \n",
      "        currentIn='in'+str(index)\n",
      "        index += 1\n",
      "        MasterDWIWorkflow.connect(all_workflows[localSessionID],'outputspec.FAImage',MergeFAsNode,currentIn)\n",
      "       \"\"\" \n",
      "   \n",
      "    ## Now do template building with FA's\n",
      "    import nipype.interfaces.ants as ants\n",
      "    \n",
      "    initAvg = pe.Node(interface=ants.AverageImages(), name ='initAvg')\n",
      "    initAvg.inputs.dimension = 3\n",
      "    initAvg.inputs.normalize = True\n",
      "    MasterDWIWorkflow.connect(myDWIWorkflow, \"outputspec.FAImage\", initAvg, \"images\")\n",
      "    \n",
      "    MergeByExtendListElementsNode = pe.Node(interface=Function(function = MergeByExtendListElements, input_names=['FAImageList'],\n",
      "                                                    output_names=['ListOfImagesDictionaries', 'registrationImageTypes', 'interpolationMapping']),\n",
      "                                                    run_without_submitting=True, name=\"99_FAMergeByExtendListElements\")\n",
      "    \n",
      "    MasterDWIWorkflow.connect(myDWIWorkflow, \"outputspec.FAImage\", MergeByExtendListElementsNode,'FAImageList')\n",
      "    ### USE ANTS REGISTRATION\n",
      "    # from nipype.workflows.smri.ants import antsRegistrationTemplateBuildSingleIterationWF\n",
      "    from BAWantsRegistrationBuildTemplate import BAWantsRegistrationTemplateBuildSingleIterationWF\n",
      "    buildTemplateIteration1 = BAWantsRegistrationTemplateBuildSingleIterationWF('iteration01')\n",
      "    ## TODO:  Change these parameters\n",
      "    BeginANTS_iter1 = buildTemplateIteration1.get_node(\"BeginANTS\")\n",
      "    BeginANTS_iter1.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 4-8 -l mem_free=9000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE_LONG), 'overwrite': True}\n",
      "    wimtdeformed_iter1 = buildTemplateIteration1.get_node(\"wimtdeformed\")\n",
      "    wimtdeformed_iter1.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 1-2 -l mem_free=2000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE), 'overwrite': True}\n",
      "    AvgAffineTransform_iter1 = buildTemplateIteration1.get_node(\"AvgAffineTransform\")\n",
      "    AvgAffineTransform_iter1.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 1 -l mem_free=2000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE), 'overwrite': True}\n",
      "    wimtPassivedeformed_iter1 = buildTemplateIteration1.get_node(\"wimtPassivedeformed\")\n",
      "    wimtPassivedeformed_iter1.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 1-2 -l mem_free=2000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE), 'overwrite': True}\n",
      "\n",
      "    MasterDWIWorkflow.connect(initAvg, 'output_average_image', buildTemplateIteration1, 'inputspec.fixed_image')\n",
      "    MasterDWIWorkflow.connect(MergeByExtendListElementsNode, 'ListOfImagesDictionaries', buildTemplateIteration1, 'inputspec.ListOfImagesDictionaries')\n",
      "    MasterDWIWorkflow.connect(MergeByExtendListElementsNode, 'registrationImageTypes', buildTemplateIteration1, 'inputspec.registrationImageTypes')\n",
      "    MasterDWIWorkflow.connect(MergeByExtendListElementsNode, 'interpolationMapping', buildTemplateIteration1, 'inputspec.interpolationMapping')\n",
      "    buildTemplateIteration2 = buildTemplateIteration1.clone(name='buildTemplateIteration2')\n",
      "    buildTemplateIteration2 = BAWantsRegistrationTemplateBuildSingleIterationWF('Iteration02')\n",
      "    ## TODO:  Change these parameters\n",
      "    BeginANTS_iter2 = buildTemplateIteration2.get_node(\"BeginANTS\")\n",
      "    BeginANTS_iter2.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 4-8 -l mem_free=9000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE_LONG), 'overwrite': True}\n",
      "    wimtdeformed_iter2 = buildTemplateIteration2.get_node(\"wimtdeformed\")\n",
      "    wimtdeformed_iter2.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 1-2 -l mem_free=2000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE), 'overwrite': True}\n",
      "    AvgAffineTransform_iter2 = buildTemplateIteration2.get_node(\"AvgAffineTransform\")\n",
      "    AvgAffineTransform_iter2.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 1 -l mem_free=2000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE), 'overwrite': True}\n",
      "    wimtPassivedeformed_iter2 = buildTemplateIteration2.get_node(\"wimtPassivedeformed\")\n",
      "    wimtPassivedeformed_iter2.plugin_args = {'template': SGE_JOB_SCRIPT, 'qsub_args': '-S /bin/bash -cwd -pe smp1 1-2 -l mem_free=2000M -o /dev/null -e /dev/null {QUEUE_OPTIONS}'.format(QUEUE_OPTIONS=CLUSTER_QUEUE), 'overwrite': True}\n",
      "\n",
      "    MasterDWIWorkflow.connect(buildTemplateIteration1, 'outputspec.template', buildTemplateIteration2, 'inputspec.fixed_image')\n",
      "    MasterDWIWorkflow.connect(MergeByExtendListElementsNode, 'ListOfImagesDictionaries', buildTemplateIteration2, 'inputspec.ListOfImagesDictionaries')\n",
      "    MasterDWIWorkflow.connect(MergeByExtendListElementsNode, 'registrationImageTypes', buildTemplateIteration2, 'inputspec.registrationImageTypes')\n",
      "    MasterDWIWorkflow.connect(MergeByExtendListElementsNode, 'interpolationMapping', buildTemplateIteration2, 'inputspec.interpolationMapping')\n",
      "    \"\"\"\n",
      "    ***********\n",
      "    \"\"\"\n",
      "    from nipype.interfaces.base import CommandLine, CommandLineInputSpec, TraitedSpec, File, Directory, traits, isdefined, InputMultiPath, OutputMultiPath\n",
      "                                       \n",
      "    import os\n",
      "\n",
      "    class ResampleDTIlogEuclideanInputSpec(CommandLineInputSpec):\n",
      "        in_file = File(argstr='%s', exists=True, mandatory=True, position=-2, desc=\"The input file for Resampledtilogeuclidean\")\n",
      "        out_file = File(argstr='%s', exists=False, mandatory=True, position=-1, desc=\"The output file for Resampledtilogeuclidean\")\n",
      "        transformationFile = File(argstr='--transformationFile %s', exists=True, mandatory=True, desc=\"The affine transformation file for Resampledtilogeuclidean\")\n",
      "        Reference = File(argstr='--Reference %s', exists=True, mandatory=True, desc=\"The reference file for ResampleDTIlogEuclidean output\")\n",
      "        defField = File(argstr='--defField %s', exists=True, mandatory=True, desc=\"The deformation file for ResampleDTIlogEuclidean output\")\n",
      "        hfieldtype = traits.Enum('hfield', 'displacement', argstr=\"--hfieldtype %s\", desc=\"Set if the deformation field is an -Field\")\n",
      "        interpolation = traits.Enum('linear', 'nn', 'ws', 'bs', argstr=\"--interpolation %s\", desc=\"Sampling algorithm\")\n",
      "        transform_tensor_method = traits.Enum('PPD', 'FS', argstr=\"--transform_tensor_method %s\", desc=\"Chooses between 2 methods to transform the tensors: Finite Strain (FS), faster but less accurate, or Preservation of the Principal Direction (PPD)\")\n",
      "\n",
      "        \n",
      "    class ResampleDTIlogEuclideanOutputSpec(TraitedSpec):\n",
      "        out_file = traits.File(exists=False, desc=\"The output file for Resampledtilogeuclidean\")\n",
      "\n",
      "\n",
      "    class ResampleDTIlogEuclidean(CommandLine):\n",
      "        _cmd = \"ResampleDTIlogEuclidean\"\n",
      "        input_spec = ResampleDTIlogEuclideanInputSpec\n",
      "        output_spec = ResampleDTIlogEuclideanOutputSpec\n",
      "\n",
      "        def _list_outputs(self):\n",
      "            outputs = self.output_spec().get()\n",
      "            outputs['out_file'] = self.inputs.out_file\n",
      "            return outputs\n",
      "\n",
      "    def getElement(inputList, index):\n",
      "        \"\"\" Custom function to get out affine and deformation fields from BeginANTS output list \"\"\"\n",
      "        return inputList[int(index)]\n",
      "    \n",
      "    ResampleDTI = pe.Node(interface=ResampleDTIlogEuclidean(), name='ResampleDTI', iterfield=['moving_image'])\n",
      "    ResampleDTI.inputs.hfieldtype = 'displacement'\n",
      "    ResampleDTI.inputs.interpolation = 'linear'\n",
      "    ResampleDTI.inputs.transform_tensor_method = 'PPD'\n",
      "    \n",
      "    MasterDWIWorkflow.connect([(buildTemplateIteration2, ResampleDTI, [(('BeginANTS.forward_transforms', getElement, 0), 'transformationFile')])]) \n",
      "    MasterDWIWorkflow.connect([(buildTemplateIteration2, ResampleDTI, [(('BeginANTS.forward_transforms', getElement, 1), 'defField')])]) \n",
      "    DTIEstimNode = myDWIWorkflow.get_node('DTIEstim_Process')\n",
      "    MasterDWIWorkflow.connect(myDWIWorkflow, 'DTIEstim_Process.tensor_output', ResampleDTI, 'Reference')\n",
      "    \n",
      "    \"\"\"\n",
      "    ***********\n",
      "    \"\"\"\n",
      "\n",
      "    DWIAverageSink = pe.Node(interface=nio.DataSink(), name=\"DWIAverageSink\")\n",
      "    \"\"\" 'Average DTIEstim_Process and average deformed DTI image' \"\"\"\n",
      "    DWIAverageSink = pe.Node(interface=nio.DataSink(), name='DWIAverageSink')\n",
      "    DWIAverageSink.inputs.base_directory = '/hjohnson/HDNI/20130214_DWIPROCESSING_NIPYPE'\n",
      "    DWIAverageSink.inputs.container = os.path.join(\"DWIPrototype_Results\")\n",
      "    # DWIAverageSink.inputs.regexp_substitutions = [('\n",
      "    \n",
      "    MasterDWIWorkflow.connect(buildTemplateIteration2, 'AvgAffineTransform.affine_transform',\n",
      "                              DWIAverageSink, 'Average.@transforms')\n",
      "    MasterDWIWorkflow.connect(buildTemplateIteration2, 'AvgWarpImages.output_average_image',\n",
      "                              DWIAverageSink, 'Average.@passive_transforms')\n",
      "    MasterDWIWorkflow.connect(buildTemplateIteration2, 'outputspec.template', \n",
      "                              DWIAverageSink, 'Average.@template')\n",
      "    MasterDWIWorkflow.connect(buildTemplateIteration2, 'outputspec.passive_deformed_templates', \n",
      "                              DWIAverageSink, 'Average.@passive')\n",
      "    \n",
      "    \n",
      "    MasterDWIWorkflow.connect(initAvg, 'output_average_image', DWIAverageSink, 'Average.@image')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/raid0/homes/johnsonhj/src/BSA-clang31/NIPYPE/nipype/interfaces/base.py:366: UserWarning: Input convergence_threshold requires inputs: number_of_iterations\n",
        "  warn(msg)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now Start Processing\n",
      "===================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multiprocessing\n",
      "total_CPUS = multiprocessing.cpu_count()\n",
      "NUMPARALLEL=1\n",
      "os.environ['NSLOTS'] = \"{0}\".format(total_CPUS / NUMPARALLEL)\n",
      "\n",
      "\n",
      "\n",
      "MasterDWIWorkflow.write_graph()\n",
      "\n",
      "SGEFlavor = 'SGE'\n",
      "\n",
      "if False:\n",
      "    MasterDWIWorkflow.run()\n",
      "else:\n",
      "    MasterDWIWorkflow.run(plugin=SGEFlavor,\n",
      "                      plugin_args=dict(template=JOB_SCRIPT,\n",
      "                      qsub_args=\"-S /bin/bash -cwd -pe smp1 1-12 -l h_vmem=19G,mem_free=2G -o /dev/null -e /dev/null \" + \"-q OSX\"))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:45,958 workflow INFO:\n",
        "\t Converting dotfile: /hjohnson/HDNI/20130214_DWIPROCESSING_NIPYPE/DWI_CACHE/DAVE_test/MasterWFname/ManySubjectDWIPrototype/graph.dot to png format\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,4 workflow INFO:\n",
        "\t ['check', 'execution', 'logging']\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,216 workflow INFO:\n",
        "\t Running in parallel.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,223 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,232 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode 99_GetDWIReferenceImagesFromSessionID\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,243 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,244 workflow INFO:\n",
        "\t Executing: _99_GetDWIReferenceImagesFromSessionID0 ID: 47\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,248 workflow INFO:\n",
        "\t [Job finished] jobname: _99_GetDWIReferenceImagesFromSessionID0 jobid: 47\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,250 workflow INFO:\n",
        "\t Executing: _99_GetDWIReferenceImagesFromSessionID1 ID: 48\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,253 workflow INFO:\n",
        "\t [Job finished] jobname: _99_GetDWIReferenceImagesFromSessionID1 jobid: 48\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,256 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,257 workflow INFO:\n",
        "\t Executing: 99_GetDWIReferenceImagesFromSessionID ID: 41\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,260 workflow INFO:\n",
        "\t [Job finished] jobname: 99_GetDWIReferenceImagesFromSessionID jobid: 41\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,267 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,301 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode B0ToT2_Rigid\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,308 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,308 workflow INFO:\n",
        "\t Executing: _B0ToT2_Rigid0 ID: 49\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,313 workflow INFO:\n",
        "\t [Job finished] jobname: _B0ToT2_Rigid0 jobid: 49\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,315 workflow INFO:\n",
        "\t Executing: _B0ToT2_Rigid1 ID: 50\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,317 workflow INFO:\n",
        "\t [Job finished] jobname: _B0ToT2_Rigid1 jobid: 50\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,321 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,321 workflow INFO:\n",
        "\t Executing: B0ToT2_Rigid ID: 19\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,325 workflow INFO:\n",
        "\t [Job finished] jobname: B0ToT2_Rigid jobid: 19\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,332 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,362 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode DWIRIP_B0ToT2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,370 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,370 workflow INFO:\n",
        "\t Executing: _DWIRIP_B0ToT20 ID: 51\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,373 workflow INFO:\n",
        "\t [Job finished] jobname: _DWIRIP_B0ToT20 jobid: 51\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,374 workflow INFO:\n",
        "\t Executing: _DWIRIP_B0ToT21 ID: 52\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,377 workflow INFO:\n",
        "\t [Job finished] jobname: _DWIRIP_B0ToT21 jobid: 52\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,380 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,381 workflow INFO:\n",
        "\t Executing: DWIRIP_B0ToT2 ID: 3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,384 workflow INFO:\n",
        "\t [Job finished] jobname: DWIRIP_B0ToT2 jobid: 3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,390 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,466 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode EXTRACT_B0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,529 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode BSPLINE_T2_TO_RIPB0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,537 workflow INFO:\n",
        "\t Submitting 4 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,537 workflow INFO:\n",
        "\t Executing: _EXTRACT_B00 ID: 53\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,539 workflow INFO:\n",
        "\t [Job finished] jobname: _EXTRACT_B00 jobid: 53\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,541 workflow INFO:\n",
        "\t Executing: _EXTRACT_B01 ID: 54\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,542 workflow INFO:\n",
        "\t [Job finished] jobname: _EXTRACT_B01 jobid: 54\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,544 workflow INFO:\n",
        "\t Executing: _BSPLINE_T2_TO_RIPB00 ID: 55\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,548 workflow INFO:\n",
        "\t [Job finished] jobname: _BSPLINE_T2_TO_RIPB00 jobid: 55\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,550 workflow INFO:\n",
        "\t Executing: _BSPLINE_T2_TO_RIPB01 ID: 56\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,553 workflow INFO:\n",
        "\t [Job finished] jobname: _BSPLINE_T2_TO_RIPB01 jobid: 56\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,556 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,556 workflow INFO:\n",
        "\t Executing: EXTRACT_B0 ID: 9\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,559 workflow INFO:\n",
        "\t [Job finished] jobname: EXTRACT_B0 jobid: 9\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,563 workflow INFO:\n",
        "\t Executing: BSPLINE_T2_TO_RIPB0 ID: 39\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,567 workflow INFO:\n",
        "\t [Job finished] jobname: BSPLINE_T2_TO_RIPB0 jobid: 39\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,573 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,717 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode RESAMPLE_BRAINMASK\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,724 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,725 workflow INFO:\n",
        "\t Executing: _RESAMPLE_BRAINMASK0 ID: 57\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,728 workflow INFO:\n",
        "\t [Job finished] jobname: _RESAMPLE_BRAINMASK0 jobid: 57\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,730 workflow INFO:\n",
        "\t Executing: _RESAMPLE_BRAINMASK1 ID: 58\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,732 workflow INFO:\n",
        "\t [Job finished] jobname: _RESAMPLE_BRAINMASK1 jobid: 58\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,737 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,737 workflow INFO:\n",
        "\t Executing: RESAMPLE_BRAINMASK ID: 6\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,741 workflow INFO:\n",
        "\t [Job finished] jobname: RESAMPLE_BRAINMASK jobid: 6\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,747 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,771 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode DTIEstim_Process\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,778 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,779 workflow INFO:\n",
        "\t Executing: _DTIEstim_Process0 ID: 59\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,781 workflow INFO:\n",
        "\t [Job finished] jobname: _DTIEstim_Process0 jobid: 59\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,783 workflow INFO:\n",
        "\t Executing: _DTIEstim_Process1 ID: 60\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,785 workflow INFO:\n",
        "\t [Job finished] jobname: _DTIEstim_Process1 jobid: 60\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,788 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,789 workflow INFO:\n",
        "\t Executing: DTIEstim_Process ID: 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,792 workflow INFO:\n",
        "\t [Job finished] jobname: DTIEstim_Process jobid: 0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,798 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,815 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode DTIProcess\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,826 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,826 workflow INFO:\n",
        "\t Executing: _DTIProcess0 ID: 61\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,831 workflow INFO:\n",
        "\t [Job finished] jobname: _DTIProcess0 jobid: 61\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,833 workflow INFO:\n",
        "\t Executing: _DTIProcess1 ID: 62\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,835 workflow INFO:\n",
        "\t [Job finished] jobname: _DTIProcess1 jobid: 62\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,838 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,839 workflow INFO:\n",
        "\t Executing: DTIProcess ID: 42\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,841 workflow INFO:\n",
        "\t [Job finished] jobname: DTIProcess jobid: 42\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,847 workflow INFO:\n",
        "\t Submitting 3 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,847 workflow INFO:\n",
        "\t Executing: DWIDataSink ID: 10\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,964 workflow INFO:\n",
        "\t Executing: initAvg ID: 36\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,973 workflow INFO:\n",
        "\t [Job finished] jobname: initAvg jobid: 36\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,980 workflow INFO:\n",
        "\t Executing: 99_FAMergeByExtendListElements ID: 38\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,987 workflow INFO:\n",
        "\t [Job finished] jobname: 99_FAMergeByExtendListElements jobid: 38\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,995 workflow INFO:\n",
        "\t Submitting 4 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:47,996 workflow INFO:\n",
        "\t Executing: 99_GetPassiveImagesNode ID: 11\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,6 workflow INFO:\n",
        "\t [Job finished] jobname: 99_GetPassiveImagesNode jobid: 11\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,11 workflow INFO:\n",
        "\t Executing: 99_GetMovingImagesNode ID: 20\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,24 workflow INFO:\n",
        "\t [Job finished] jobname: 99_GetMovingImagesNode jobid: 20\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,29 workflow INFO:\n",
        "\t Executing: 99_GetPassiveImagesNode ID: 29\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,38 workflow INFO:\n",
        "\t [Job finished] jobname: 99_GetPassiveImagesNode jobid: 29\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,42 workflow INFO:\n",
        "\t Executing: 99_GetMovingImagesNode ID: 45\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,55 workflow INFO:\n",
        "\t [Job finished] jobname: 99_GetMovingImagesNode jobid: 45\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,60 workflow INFO:\n",
        "\t Submitting 1 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,95 workflow INFO:\n",
        "\t Adding 2 jobs for mapnode BeginANTS\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,106 workflow INFO:\n",
        "\t Submitting 2 jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,107 workflow INFO:\n",
        "\t Executing: _BeginANTS0 ID: 63\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:07:48,182 workflow INFO:\n",
        "\t Executing: _BeginANTS1 ID: 64\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130225-18:08:15,318 workflow INFO:\n",
        "\t [Job finished] jobname: DWIDataSink jobid: 10\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sys.argv\n",
      "print sys.api_version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}